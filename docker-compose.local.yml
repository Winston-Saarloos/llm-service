services:
  llm-service:
    build: .
    ports:
      - "6002:6002"
    env_file:
      - .env
    # Connect to existing Ollama container running on the host
    extra_hosts:
      - "host.docker.internal:host-gateway"
  embedding-worker:
    build: .
    command: ["python", "scripts/embedding_worker.py"]
    env_file:
      - .env
    extra_hosts:
      - "host.docker.internal:host-gateway"
