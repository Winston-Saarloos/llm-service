services:
  llm-service:
    build: .
    ports:
      - "6002:6002"
    env_file:
      - .env
    # Connect to existing Ollama container running on the host
    extra_hosts:
      - "host.docker.internal:host-gateway"
  embedding-worker:
    build: .
    command: ["python", "scripts/embedding_worker.py"]
    env_file:
      - .env
    environment:
      - LLM_SERVICE_URL=${LLM_SERVICE_URL:-http://llm-service:6002}
    extra_hosts:
      - "host.docker.internal:host-gateway"
    depends_on:
      - llm-service
