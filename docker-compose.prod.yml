services:
  llm-service:
    image: llm-service:main
    container_name: llm-service
    restart: unless-stopped
    ports:
      - "6002:6002"
    environment:
      - OLLAMA_HOST=http://host.docker.internal:11434
      - SERVER_AUTH_SECRET=${SERVER_AUTH_SECRET}
  embedding-worker:
    image: llm-service:main
    container_name: embedding-worker
    restart: unless-stopped
    command: ["python", "scripts/embedding_worker.py"]
    environment:
      - LLM_SERVICE_URL=${LLM_SERVICE_URL:-http://llm-service:6002}
      - SERVER_AUTH_SECRET=${SERVER_AUTH_SECRET}
      - REDIS_URL=${REDIS_URL}
      - MEMORY_SERVICE_URL=${MEMORY_SERVICE_URL}
      - MEMORY_SERVICE_SECRET=${MEMORY_SERVICE_SECRET}
      - EMBEDDING_MODEL=mxbai-embed-large:335m
      - EMBEDDING_CHUNK_TOKENS=350
      - EMBEDDING_CHUNK_OVERLAP_TOKENS=60
    depends_on:
      - llm-service
